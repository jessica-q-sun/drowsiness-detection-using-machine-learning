# -*- coding: utf-8 -*-
"""Final_System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16ShT9Abr5iHcSEVCwBKAzZ_fqw95-GJL

### **Detecting Driver Drowsiness Using Machine Learning**

Jessica Sun,  Sep. 19, 2020

Driver fatigue is one of the major causes of vehicle crashes. This project builds a neural network that detects signs of drowsiness and classifies the driver as alert or drowsy. 

**Data**

Data are collected from The University of Texas at Arlington Real-Life Drowsiness Dataset (UTA-RLDD): https://sites.google.com/view/utarldd/home. From the dataset, I was able to extract 16,320 frames with 34 participants with a diversity of race, gender, and facial features (glasses, facial hair, etc.). Participants self recorded the videos at the time when they felt alert or drowsy. Next, I extracted facial marks and computed features, including pupil diameter, eye aspect ratio, head tilt angle, etc.

**Model**

The CNN has three main layers and achieved an accuracy of 0.89 and validated accuracy of 0.72, which is a more than 15% improvement in preduction accuracy compared to the baseline model.

**Source**

Various sources and literature are incorporated in the project.

https://openaccess.thecvf.com/content_CVPRW_2019/papers/AMFG/Ghoddoosian_A_Realistic_Dataset_and_Baseline_Temporal_Model_for_Early_Drowsiness_CVPRW_2019_paper.pdf - Dataset on real life drowsiness detection

https://link.springer.com/article/10.1186/s40535-018-0054-9#:~:text=Videos%20are%20preprocessed%20and%20several,and%20head%20rotation%20(ROT).&text=A%20recent%20study%20used%20on,regions%20to%20detect%20driver%20drowsiness. Feature selection for driving fatigue

https://github.com/sandyying/APM-Drowsiness-Detection - Extract frames and face landmarks and build neural networks

https://towardsdatascience.com/drowsiness-detection-with-machine-learning-765a16ca208a Drowsiness detection with machine learning
"""

# load library
import dlib
import cv2
from imutils import face_utils
from scipy.spatial import distance 
import math
import pandas as pd
import numpy as np
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPool1D, Dropout, Activation
from tensorflow.keras import Model, datasets, layers, models
from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, roc_auc_score, f1_score
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.tree import DecisionTreeClassifier
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn import metrics
import warnings
from sklearn import preprocessing

p = 'drive/My Drive/DriverData/shape_predictor_68_face_landmarks.dat'
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor(p)

# mount to google drive
from google.colab import drive
drive.mount('/content/drive/')

"""# **1. Define key features to detect drowsiness**"""

def pupil_diameter(data):
  left_eye_diameter = distance.euclidean(data[37], data[40])
  right_eye_diatmer = distance.euclidean(data[44], data[47])
  return (left_eye_diameter + right_eye_diatmer)/2

def eye_aspect_ratio(data):
  left_eye_D1 = distance.euclidean(data[37], data[41])
  left_eye_D2 = distance.euclidean(data[38], data[40])
  left_eye_D3 = distance.euclidean(data[36], data[41])
  left_eye_ratio = (left_eye_D1 + left_eye_D2) / (2 * left_eye_D3)
  right_eye_D1 = distance.euclidean(data[43], data[47])
  right_eye_D2 = distance.euclidean(data[44], data[46])
  right_eye_D3 = distance.euclidean(data[42], data[45])
  right_eye_ratio = (right_eye_D1 + right_eye_D2) / (2 * right_eye_D3)
  return (left_eye_ratio + right_eye_ratio)/2

def head_tilt_degree(data):
  left_eye_left_corner = data[36]
  left_eye_right_corner = data[39]
  left_eye_tilt_rad =  math.atan2(left_eye_left_corner[1]-left_eye_right_corner[1], left_eye_right_corner[0]-left_eye_left_corner[0])
  left_eye_tilt = abs(math.degrees(left_eye_tilt_rad))

  right_eye_left_corner = data[42]
  right_eye_right_corner = data[45]
  right_eye_tilt_rad =  math.atan2(right_eye_left_corner[1]-right_eye_right_corner[1], right_eye_right_corner[0]-right_eye_left_corner[0])
  right_eye_tilt = abs(math.degrees(right_eye_tilt_rad))
  return (left_eye_tilt + right_eye_tilt)/2

def mouth_aspect_ratio(data):
    mouth_height = distance.euclidean(data[51], data[57])
    mouth_width = distance.euclidean(data[48], data[54])
    return mouth_height / mouth_width

def nasal_flare(data):
  left_nasal = distance.euclidean(data[31], data[32])
  right_nasal = distance.euclidean(data[34], data[35])
  return (left_nasal + right_nasal)/2

def mouth_to_eye(data):
  mouth_to_left_eye = distance.euclidean(data[57], data[39])
  mouth_to_right_eye = distance.euclidean(data[57], data[42])
  return (mouth_to_left_eye + mouth_to_right_eye)/2

"""# **2. Read training data**"""

df = pd.read_csv('drive/My Drive/DriverData/driver_final_data.csv')
df = df.drop(df.columns[0],axis=1)
df.loc[df['Y'] == 10, 'Y'] = 1
train_size = 23
test_size = 11
train_data = df[:train_size*480]
test_data = df[-test_size*480:]
X_train = train_data.drop('Y',axis=1)
y_train = train_data['Y']
X_test = test_data.drop(["Y"],axis=1)
y_test = test_data["Y"]

X_train_shaped = np.expand_dims(X_train, axis=2)
X_test_shaped = np.expand_dims(X_test, axis=2)

"""# **3. Build neural network**"""

model = models.Sequential()

# First layer
model.add(layers.Conv1D(64, kernel_size = 3, activation = 'relu', input_shape = (12,1)))
model.add(layers.MaxPooling1D((1)))

# Second layer
model.add(layers.Conv1D(32, kernel_size = 3, activation = 'relu'))
model.add(Flatten())

# Third layer
model.add(Dense(64, input_dim=20, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1,activation = 'sigmoid'))

model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00001),
              loss = 'binary_crossentropy',
              metrics = ['accuracy'])
history = model.fit(X_train_shaped, y_train, validation_data = (X_test_shaped,y_test), epochs = 100)

def model(data):

    features = pd.DataFrame(columns=["pupil","eye_ratio","head_tilt","mouth_ratio","nasal","mouth_eye"])
    pupil = pupil_diameter(data)
    eye_ratio = eye_aspect_ratio(data)
    head_tilt = head_tilt_degree(data)
    mouth_ratio = mouth_aspect_ratio(data)
    nasal = nasal_flare(data)
    mouth_eye = mouth_to_eye(data)
    df = features.append({"pupil":pupil,"eye_ratio": eye_ratio,"head_tilt": head_tilt,"mouth_ratio": mouth_ratio,"nasal": nasal,"mouth_eye": mouth_eye},ignore_index=True)

    df["pupil_N"] = (df["pupil"]-mean["pupil"])/ std["pupil"]
    df["eye_ratio_N"] = (df["eye_ratio"]-mean["eye_ratio"])/ std["eye_ratio"]

    df["head_tilt_N"] = (df["head_tilt"]-mean["head_tilt"])/ std["head_tilt"]
    df["mouth_ratio_N"] = (df["mouth_ratio"]-mean["mouth_ratio"])/ std["mouth_ratio"]

    df["nasal_N"] = (df["nasal"]-mean["nasal"])/ std["nasal"]
    df["mouth_eye_N"] = (df["mouth_eye"]-mean["mouth_eye"])/ std["mouth_eye"]
    
    Result = model.predict(df)
    if Result == 1:
        Result_String = "Drowsy"
    else:
        Result_String = "Alert"
  
    return Result_String, df.values

"""# **4. Capture live image from webcam**"""

from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

try:
  filename = take_photo()
  print('Saved to {}'.format(filename)) 
  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

"""# **5. Display the image and perform detection**"""

def calibration():
    data = []
    cap = cv2.VideoCapture(0)

    while True:
        # Getting out image by webcam 
        _, image = cap.read()
        # Converting the image to gray scale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # Get faces into webcam's image
        rects = detector(image, 0)

        # For each detected face, find the landmark.
        for (i, rect) in enumerate(rects):
            # Make the prediction and transfom it to numpy array
            shape = predictor(gray, rect)
            shape = face_utils.shape_to_np(shape)
            data.append(shape)
            cv2.putText(image,"Calibrating...", bottomLeftCornerOfText, font, fontScale, fontColor,lineType)

            # Draw on our image, all the finded cordinate points (x,y) 
            for (x, y) in shape:
                cv2.circle(image, (x, y), 2, (0, 255, 0), -1)

        # Show the image
        cv2.imshow("Output", image)

        k = cv2.waitKey(5) & 0xFF
        if k == 27:
            break

    cv2.destroyAllWindows()
    cap.release()
    
    
    features_test = []
    for d in data:
        eye = d[36:68]
        ear = eye_aspect_ratio(eye)
        mar = mouth_aspect_ratio(eye)
        cir = circularity(eye)
        mouth_eye = mouth_over_eye(eye)
        features_test.append([ear, mar, cir, mouth_eye])
    
    features_test = np.array(features_test)
    x = features_test
    y = pd.DataFrame(x,columns=["EAR","MAR","Circularity","MOE"])
    df_means = y.mean(axis=0)
    df_std = y.std(axis=0)
    
    return df_means,df_std

font                   = cv2.FONT_HERSHEY_SIMPLEX
bottomLeftCornerOfText = (10,400)
fontScale              = 1
fontColor              = (255,255,255)
lineType               = 2

def live():
    cap = cv2.VideoCapture(0)
    data = []
    result = []
    while True:
        # Getting out image by webcam 
        _, image = cap.read()
        # Converting the image to gray scale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # Get faces into webcam's image
        rects = detector(image, 0)

        # For each detected face, find the landmark.
        for (i, rect) in enumerate(rects):
            # Make the prediction and transfom it to numpy array
            shape = predictor(gray, rect)
            shape = face_utils.shape_to_np(shape)
            Result_String, features = model(shape)
            cv2.putText(image,Result_String, bottomLeftCornerOfText, font, fontScale, fontColor,lineType)
            data.append (features)
            result.append(Result_String)

            # Draw on our image, all the finded cordinate points (x,y) 
            for (x, y) in shape:
                cv2.circle(image, (x, y), 2, (0, 255, 0), -1)

        # Show the image
        cv2.imshow("Output", image)

        k = cv2.waitKey(300) & 0xFF
        if k == 27:
            break

    cv2.destroyAllWindows()
    cap.release()
    return data,result
font                   = cv2.FONT_HERSHEY_SIMPLEX
bottomLeftCornerOfText = (10,400)
fontScale              = 1
fontColor              = (255,255,255)
lineType               = 2